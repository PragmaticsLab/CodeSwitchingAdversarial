{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T10:47:26.685066Z",
     "start_time": "2021-05-16T10:47:25.598362Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import load_config\n",
    "from dataset import read_atis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "data_path = '/home/lesha/diploma/pieces of paper/reports/main/tables/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T10:47:26.852363Z",
     "start_time": "2021-05-16T10:47:26.686041Z"
    }
   },
   "outputs": [],
   "source": [
    "test = read_atis('test', ['en'])\n",
    "\n",
    "from adversarial import AdversarialWordLevel, AdversarialAlignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.861662Z",
     "start_time": "2021-05-15T18:57:18.858121Z"
    }
   },
   "outputs": [],
   "source": [
    "label_num = 0\n",
    "\n",
    "def df_to_latex(df, caption: str = None):\n",
    "    \n",
    "    def array_fixer(x):\n",
    "        if isinstance(x, list) or isinstance(x, np.ndarray):\n",
    "            return f'${np.mean(x):.3f} \\pm {np.std(x):.3f}$'\n",
    "        else:\n",
    "            return f'${x:.3f}$'\n",
    "    \n",
    "    result = \"\"\"\\\n",
    "\\\\begin{{table}}[H]\n",
    "\\t\\\\resizebox{{\\\\textwidth}}{{!}}{{\n",
    "\\t\\t\\\\begin{{tabular}}{{|>{{\\\\bfseries}}l|{}}}\n",
    "\\t\\t\\t\\\\hline\n",
    "{}\n",
    "\\t\\t\\\\end{{tabular}}\n",
    "\\t}}{}\n",
    "\\\\end{{table}}\\\n",
    "\"\"\"\n",
    "\n",
    "    columns = 'c|' * df.shape[1]\n",
    "        \n",
    "    body = ['& ' + ' & '.join(df.columns)]\n",
    "    \n",
    "    body += [\n",
    "        df.index[i] +\n",
    "        '&' + \n",
    "        ' & '.join(map(array_fixer, df.iloc[i].values)) \n",
    "        for i in range(len(df))\n",
    "    ]\n",
    "    \n",
    "    for i in range(len(body)):\n",
    "        body[i] = '\\t' * 3 + body[i] + ' \\\\\\\\ \\\\hline'\n",
    "        \n",
    "\n",
    "    body = '\\n'.join(body).replace('_', '\\\\_')\n",
    "    \n",
    "    if caption is not None:\n",
    "        caption = f'\\caption{{{caption}}}'\n",
    "    else:\n",
    "        caption = ''\n",
    "        \n",
    "    global label_num\n",
    "        \n",
    "    caption += f'\\\\label{{tab:table{label_num}}}'\n",
    "\n",
    "    label_num += 1\n",
    "\n",
    "    result = result.format(columns, body, caption)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.873113Z",
     "start_time": "2021-05-15T18:57:18.862475Z"
    }
   },
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'xlm-r',\n",
    "    'm-bert',\n",
    "    'xlm-r en',\n",
    "    'm-bert en',\n",
    "    'xlm-r adv',\n",
    "    'm-bert adv',\n",
    "    'xlm-r en + adv',\n",
    "    'm-bert en + adv'\n",
    "]\n",
    "\n",
    "model_args = [\n",
    "    (False, False),\n",
    "    (False, False),\n",
    "    (True, False),\n",
    "    (True, False),\n",
    "    (False, True),\n",
    "    (False, True),\n",
    "    (True, True),\n",
    "    (True, True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.877719Z",
     "start_time": "2021-05-15T18:57:18.873756Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_attacks(language, model_name, only_english: bool = False, adv_pretrained: bool = False):\n",
    "    return pd.read_csv(\n",
    "        f'results/{language}/{model_name}_{int(only_english)}_{int(adv_pretrained)}.csv',\n",
    "        index_col=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.882381Z",
     "start_time": "2021-05-15T18:57:18.878412Z"
    }
   },
   "outputs": [],
   "source": [
    "index_renamer = {\n",
    "    'intent_acc': 'Intent accuracy',\n",
    "    'slot_f1': 'Slots F1 score',\n",
    "    'sementic_frame_acc': 'Semantic accuracy',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.904242Z",
     "start_time": "2021-05-15T18:57:18.883099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intent accuracy':                  en        de        es        fr        ja        pt  \\\n",
       " xlm-r      0.980132  0.976159  0.966887  0.970861  0.969536  0.966887   \n",
       " m-bert     0.978808  0.976159  0.957616  0.968212  0.954967  0.968212   \n",
       " xlm-r en   0.901987  0.875497  0.878146  0.879470  0.785430  0.774834   \n",
       " m-bert en  0.952318  0.819868  0.870199  0.875497  0.747020  0.838411   \n",
       " \n",
       "                  zh       avg  \n",
       " xlm-r      0.964238  0.970672  \n",
       " m-bert     0.956291  0.965752  \n",
       " xlm-r en   0.847682  0.849007  \n",
       " m-bert en  0.815894  0.845601  ,\n",
       " 'Slots F1 score':                  en        de        es        fr        ja        pt  \\\n",
       " xlm-r      0.943711  0.938557  0.907517  0.924231  0.928918  0.923868   \n",
       " m-bert     0.947356  0.945361  0.885226  0.925663  0.935154  0.924304   \n",
       " xlm-r en   0.870407  0.668776  0.751045  0.612022  0.573139  0.672652   \n",
       " m-bert en  0.899340  0.557940  0.783145  0.534114  0.621519  0.518100   \n",
       " \n",
       "                  zh       avg  \n",
       " xlm-r      0.941942  0.929821  \n",
       " m-bert     0.944663  0.929675  \n",
       " xlm-r en   0.737900  0.697992  \n",
       " m-bert en  0.679905  0.656295  ,\n",
       " 'Semantic accuracy':                  en        de        es        fr        ja        pt  \\\n",
       " xlm-r      0.826490  0.826490  0.696689  0.806623  0.739073  0.800000   \n",
       " m-bert     0.854305  0.854305  0.652980  0.803974  0.740397  0.807947   \n",
       " xlm-r en   0.558940  0.250331  0.347020  0.150993  0.000000  0.203974   \n",
       " m-bert en  0.671523  0.188079  0.401325  0.194702  0.051656  0.177483   \n",
       " \n",
       "                  zh       avg  \n",
       " xlm-r      0.778808  0.782025  \n",
       " m-bert     0.796026  0.787133  \n",
       " xlm-r en   0.132450  0.234816  \n",
       " m-bert en  0.213245  0.271145  }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# СРАВНЕНИЕ МОДЕЛЕЙ МЕЖДУ СОБОЙ ПРОСТО НА ТЕСТОВОЙ ВЫБОРКЕ (БЕЗ ЗАЩИТЫ)\n",
    "\n",
    "output = {index_renamer[key]: {} for key in index_renamer}\n",
    "\n",
    "for model_name, model_arg in zip(model_names[:4], model_args):\n",
    "    df = get_model_attacks('test', model_name.split()[0], *model_arg)\n",
    "\n",
    "    for key in index_renamer.keys():\n",
    "        values = df[key].to_dict()\n",
    "        values['avg'] = np.mean(list(values.values()))\n",
    "        output[index_renamer[key]][model_name] = values\n",
    "\n",
    "output = {key: pd.DataFrame.from_dict(output[key]).rename(index=index_renamer).transpose() for key in output.keys()}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.908144Z",
     "start_time": "2021-05-15T18:57:18.905410Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + '1.tex', 'w') as f:\n",
    "    for key in output.keys():\n",
    "        print(\n",
    "            df_to_latex(\n",
    "                output[key],\n",
    "                f'Сравнение моделей между собой на тестовой выборке датасета MultiAtis++ по метрике \\\\textbf{{{key}}}. По колонкам языки тестовых подвыборок, по рядам тестируемые модели.'\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.927366Z",
     "start_time": "2021-05-15T18:57:18.908959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intent accuracy':                  de        es        fr        ja        pt        zh  \\\n",
       " xlm-r      0.931126  0.876821  0.849007  0.825166  0.900662  0.871523   \n",
       " m-bert     0.892715  0.891391  0.871523  0.819868  0.852980  0.851656   \n",
       " xlm-r en   0.809272  0.782781  0.773510  0.676821  0.553642  0.728477   \n",
       " m-bert en  0.810596  0.760265  0.793377  0.723179  0.760265  0.777483   \n",
       " \n",
       "                 avg  \n",
       " xlm-r      0.875717  \n",
       " m-bert     0.863355  \n",
       " xlm-r en   0.720751  \n",
       " m-bert en  0.770861  ,\n",
       " 'Slots F1 score':                  de        es        fr        ja        pt        zh  \\\n",
       " xlm-r      0.766675  0.588813  0.602636  0.551503  0.597709  0.746898   \n",
       " m-bert     0.685462  0.516985  0.509909  0.427834  0.494069  0.684211   \n",
       " xlm-r en   0.642416  0.466905  0.499416  0.508277  0.542569  0.640907   \n",
       " m-bert en  0.539088  0.385202  0.419195  0.362173  0.390605  0.585037   \n",
       " \n",
       "                 avg  \n",
       " xlm-r      0.642372  \n",
       " m-bert     0.553078  \n",
       " xlm-r en   0.550082  \n",
       " m-bert en  0.446883  ,\n",
       " 'Semantic accuracy':                  de        es        fr        ja        pt        zh  \\\n",
       " xlm-r      0.343046  0.108609  0.086093  0.169536  0.076821  0.278146   \n",
       " m-bert     0.227815  0.083444  0.058278  0.080795  0.038411  0.213245   \n",
       " xlm-r en   0.201325  0.056954  0.055629  0.013245  0.022517  0.042384   \n",
       " m-bert en  0.136424  0.029139  0.031788  0.005298  0.010596  0.115232   \n",
       " \n",
       "                 avg  \n",
       " xlm-r      0.177042  \n",
       " m-bert     0.116998  \n",
       " xlm-r en   0.065342  \n",
       " m-bert en  0.054746  }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# СРАВНЕНИЕ МОДЕЛЕЙ МЕЖДУ СОБОЙ ПО АТАКЕ WORD LEVEL\n",
    "\n",
    "output = {index_renamer[key]: {} for key in index_renamer}\n",
    "\n",
    "for model_name, model_arg in zip(model_names[:4], model_args):\n",
    "    df = get_model_attacks('en', model_name.split()[0], *model_arg)\n",
    "\n",
    "    for key in index_renamer.keys():\n",
    "        mask = df.index.map(lambda x: 'Word' in x)\n",
    "        values = df[mask][key].to_dict()\n",
    "        values['[avg]'] = np.mean(list(values.values()))\n",
    "        output[index_renamer[key]][model_name] = {key_[key_.find('[') + 1:key_.find(']')]: values[key_] for key_ in values.keys()}\n",
    "\n",
    "output = {key: pd.DataFrame.from_dict(output[key]).rename(index=index_renamer).transpose() for key in output.keys()}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.930698Z",
     "start_time": "2021-05-15T18:57:18.928084Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + '2.tex', 'w') as f:\n",
    "    for key in output.keys():\n",
    "        print(\n",
    "            df_to_latex(\n",
    "                output[key],\n",
    "                f'Сравнение моделей между собой после word-level атаки на тестовую выборку датасета MultiAtis++ по метрике \\\\textbf{{{key}}}. По колонкам встраиваемые языки, по рядам тестируемые модели.'\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.951374Z",
     "start_time": "2021-05-15T18:57:18.931391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intent accuracy':                  de        es        fr        ja        pt        zh  \\\n",
       " xlm-r      0.953642  0.945695  0.928477  0.952318  0.964238  0.949669   \n",
       " m-bert     0.948344  0.935099  0.939073  0.950993  0.940397  0.933775   \n",
       " xlm-r en   0.807947  0.835762  0.740397  0.749669  0.442384  0.784106   \n",
       " m-bert en  0.809272  0.833113  0.834437  0.805298  0.860927  0.829139   \n",
       " \n",
       "                 avg  \n",
       " xlm-r      0.949007  \n",
       " m-bert     0.941280  \n",
       " xlm-r en   0.726711  \n",
       " m-bert en  0.828698  ,\n",
       " 'Slots F1 score':                  de        es        fr        ja        pt       zh       avg\n",
       " xlm-r      0.801866  0.829175  0.750731  0.443746  0.813370  0.60896  0.707975\n",
       " m-bert     0.784309  0.803944  0.758233  0.449928  0.782991  0.61937  0.699796\n",
       " xlm-r en   0.626586  0.704014  0.568657  0.364769  0.679594  0.56056  0.584030\n",
       " m-bert en  0.539165  0.698585  0.531226  0.366416  0.530324  0.56319  0.538151,\n",
       " 'Semantic accuracy':                  de        es        fr        ja        pt        zh  \\\n",
       " xlm-r      0.511258  0.511258  0.336424  0.115232  0.521854  0.209272   \n",
       " m-bert     0.487417  0.438411  0.344371  0.113907  0.433113  0.255629   \n",
       " xlm-r en   0.162914  0.229139  0.099338  0.013245  0.070199  0.063576   \n",
       " m-bert en  0.121854  0.218543  0.084768  0.041060  0.087417  0.105960   \n",
       " \n",
       "                 avg  \n",
       " xlm-r      0.367550  \n",
       " m-bert     0.345475  \n",
       " xlm-r en   0.106402  \n",
       " m-bert en  0.109934  }"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# СРАВНЕНИЕ МОДЕЛЕЙ МЕЖДУ СОБОЙ ПО АТАКЕ ALIGNMENTS\n",
    "\n",
    "output = {index_renamer[key]: {} for key in index_renamer}\n",
    "\n",
    "for model_name, model_arg in zip(model_names[:4], model_args):\n",
    "    df = get_model_attacks('en', model_name.split()[0], *model_arg)\n",
    "\n",
    "    for key in index_renamer.keys():\n",
    "        mask = df.index.map(lambda x: 'Align' in x)\n",
    "        values = df[mask][key].to_dict()\n",
    "        values['[avg]'] = np.mean(list(values.values()))\n",
    "        output[index_renamer[key]][model_name] = {key_[key_.find('[') + 1:key_.find(']')]: values[key_] for key_ in values.keys()}\n",
    "\n",
    "output = {key: pd.DataFrame.from_dict(output[key]).rename(index=index_renamer).transpose() for key in output.keys()}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.954758Z",
     "start_time": "2021-05-15T18:57:18.952039Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + '3.tex', 'w') as f:\n",
    "    for key in output.keys():\n",
    "        print(\n",
    "            df_to_latex(\n",
    "                output[key],\n",
    "                f'Сравнение моделей между собой после phrase-level атаки на тестовую выборку датасета MultiAtis++ по метрике \\\\textbf{{{key}}}. По колонкам встраиваемые языки, по рядам тестируемые модели.'\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.971671Z",
     "start_time": "2021-05-15T18:57:18.955393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intent accuracy':                        en        de        es        fr        ja        pt  \\\n",
       " xlm-r adv        0.981457  0.973510  0.964238  0.976159  0.972185  0.966887   \n",
       " m-bert adv       0.974834  0.976159  0.964238  0.972185  0.960265  0.969536   \n",
       " xlm-r en + adv   0.928477  0.890066  0.912583  0.871523  0.789404  0.880795   \n",
       " m-bert en + adv  0.958940  0.847682  0.900662  0.892715  0.719205  0.900662   \n",
       " \n",
       "                        zh       avg  \n",
       " xlm-r adv        0.966887  0.971618  \n",
       " m-bert adv       0.961589  0.968401  \n",
       " xlm-r en + adv   0.815894  0.869820  \n",
       " m-bert en + adv  0.758940  0.854115  ,\n",
       " 'Slots F1 score':                        en        de        es        fr        ja        pt  \\\n",
       " xlm-r adv        0.946577  0.940163  0.905585  0.929329  0.927940  0.928786   \n",
       " m-bert adv       0.950394  0.942157  0.900359  0.927521  0.935022  0.920078   \n",
       " xlm-r en + adv   0.888136  0.729185  0.788215  0.622503  0.447176  0.742906   \n",
       " m-bert en + adv  0.899547  0.565747  0.759195  0.556687  0.416033  0.553678   \n",
       " \n",
       "                        zh       avg  \n",
       " xlm-r adv        0.945508  0.931984  \n",
       " m-bert adv       0.946142  0.931668  \n",
       " xlm-r en + adv   0.717868  0.705141  \n",
       " m-bert en + adv  0.603783  0.622096  ,\n",
       " 'Semantic accuracy':                        en        de        es        fr        ja        pt  \\\n",
       " xlm-r adv        0.833113  0.831788  0.692715  0.813245  0.745695  0.809272   \n",
       " m-bert adv       0.860927  0.854305  0.682119  0.805298  0.737748  0.798675   \n",
       " xlm-r en + adv   0.613245  0.397351  0.403974  0.108609  0.005298  0.418543   \n",
       " m-bert en + adv  0.674172  0.266225  0.365563  0.264901  0.003974  0.278146   \n",
       " \n",
       "                        zh       avg  \n",
       " xlm-r adv        0.792053  0.788269  \n",
       " m-bert adv       0.797351  0.790918  \n",
       " xlm-r en + adv   0.136424  0.297635  \n",
       " m-bert en + adv  0.136424  0.284201  }"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# СРАВНЕНИЕ МОДЕЛЕЙ МЕЖДУ СОБОЙ ПРОСТО НА ТЕСТОВОЙ ВЫБОРКЕ (С ЗАЩИТОЙ)\n",
    "\n",
    "output = {index_renamer[key]: {} for key in index_renamer}\n",
    "\n",
    "for model_name, model_arg in zip(model_names[4:], model_args[4:]):\n",
    "    df = get_model_attacks('test', model_name.split()[0], *model_arg)\n",
    "\n",
    "    for key in index_renamer.keys():\n",
    "        values = df[key].to_dict()\n",
    "        values['avg'] = np.mean(list(values.values()))\n",
    "        output[index_renamer[key]][model_name] = values\n",
    "\n",
    "output = {key: pd.DataFrame.from_dict(output[key]).rename(index=index_renamer).transpose() for key in output.keys()}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.974991Z",
     "start_time": "2021-05-15T18:57:18.972358Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + '4.tex', 'w') as f:\n",
    "    for key in output.keys():\n",
    "        print(\n",
    "            df_to_latex(\n",
    "                output[key],\n",
    "                f'Сравнение моделей с защитой между собой на тестовой выборке датасета MultiAtis++ по метрике \\\\textbf{{{key}}}. По колонкам языки тестовых подвыборок, по рядам тестируемые модели.'\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.994276Z",
     "start_time": "2021-05-15T18:57:18.975620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intent accuracy':                        de        es        fr        ja        pt        zh  \\\n",
       " xlm-r adv        0.935099  0.884768  0.895364  0.838411  0.917881  0.856954   \n",
       " m-bert adv       0.923179  0.894040  0.890066  0.866225  0.899338  0.884768   \n",
       " xlm-r en + adv   0.842384  0.817219  0.811921  0.613245  0.810596  0.720530   \n",
       " m-bert en + adv  0.864901  0.827815  0.854305  0.760265  0.855629  0.749669   \n",
       " \n",
       "                       avg  \n",
       " xlm-r adv        0.888079  \n",
       " m-bert adv       0.892936  \n",
       " xlm-r en + adv   0.769316  \n",
       " m-bert en + adv  0.818764  ,\n",
       " 'Slots F1 score':                        de        es        fr        ja        pt        zh  \\\n",
       " xlm-r adv        0.768449  0.608536  0.590965  0.519163  0.607991  0.743609   \n",
       " m-bert adv       0.703900  0.531871  0.534875  0.470119  0.562741  0.685425   \n",
       " xlm-r en + adv   0.648408  0.509328  0.508400  0.455320  0.541513  0.662532   \n",
       " m-bert en + adv  0.531093  0.404718  0.371309  0.440516  0.417090  0.563148   \n",
       " \n",
       "                       avg  \n",
       " xlm-r adv        0.639785  \n",
       " m-bert adv       0.581489  \n",
       " xlm-r en + adv   0.554250  \n",
       " m-bert en + adv  0.454646  ,\n",
       " 'Semantic accuracy':                        de        es        fr        ja        pt        zh  \\\n",
       " xlm-r adv        0.347020  0.133775  0.092715  0.099338  0.088742  0.283444   \n",
       " m-bert adv       0.301987  0.113907  0.087417  0.120530  0.070199  0.237086   \n",
       " xlm-r en + adv   0.235762  0.094040  0.064901  0.011921  0.072848  0.128477   \n",
       " m-bert en + adv  0.186755  0.072848  0.035762  0.043709  0.038411  0.109934   \n",
       " \n",
       "                       avg  \n",
       " xlm-r adv        0.174172  \n",
       " m-bert adv       0.155188  \n",
       " xlm-r en + adv   0.101325  \n",
       " m-bert en + adv  0.081236  }"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# СРАВНЕНИЕ МОДЕЛЕЙ МЕЖДУ СОБОЙ ПО АТАКЕ WORD LEVEL\n",
    "\n",
    "output = {index_renamer[key]: {} for key in index_renamer}\n",
    "\n",
    "for model_name, model_arg in zip(model_names[4:], model_args[4:]):\n",
    "    df = get_model_attacks('en', model_name.split()[0], *model_arg)\n",
    "\n",
    "    for key in index_renamer.keys():\n",
    "        mask = df.index.map(lambda x: 'Word' in x)\n",
    "        values = df[mask][key].to_dict()\n",
    "        values['[avg]'] = np.mean(list(values.values()))\n",
    "        output[index_renamer[key]][model_name] = {key_[key_.find('[') + 1:key_.find(']')]: values[key_] for key_ in values.keys()}\n",
    "\n",
    "output = {key: pd.DataFrame.from_dict(output[key]).rename(index=index_renamer).transpose() for key in output.keys()}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:18.997565Z",
     "start_time": "2021-05-15T18:57:18.994959Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + '5.tex', 'w') as f:\n",
    "    for key in output.keys():\n",
    "        print(\n",
    "            df_to_latex(\n",
    "                output[key],\n",
    "                f'Сравнение моделей с защитой между собой после word-level атаки на тестовую выборку датасета MultiAtis++ по метрике \\\\textbf{{{key}}}. По колонкам встраиваемые языки, по рядам тестируемые модели.'\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:19.018221Z",
     "start_time": "2021-05-15T18:57:18.998238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intent accuracy':                        de        es        fr        ja        pt        zh  \\\n",
       " xlm-r adv        0.958940  0.957616  0.929801  0.960265  0.957616  0.947020   \n",
       " m-bert adv       0.956291  0.949669  0.944371  0.957616  0.954967  0.945695   \n",
       " xlm-r en + adv   0.870199  0.856954  0.810596  0.794702  0.852980  0.776159   \n",
       " m-bert en + adv  0.846358  0.890066  0.892715  0.765563  0.900662  0.784106   \n",
       " \n",
       "                       avg  \n",
       " xlm-r adv        0.951876  \n",
       " m-bert adv       0.951435  \n",
       " xlm-r en + adv   0.826932  \n",
       " m-bert en + adv  0.846578  ,\n",
       " 'Slots F1 score':                        de        es        fr        ja        pt        zh  \\\n",
       " xlm-r adv        0.808616  0.847312  0.770889  0.431993  0.820305  0.617295   \n",
       " m-bert adv       0.807282  0.848049  0.790042  0.445657  0.822081  0.635233   \n",
       " xlm-r en + adv   0.682781  0.775646  0.647887  0.326389  0.721132  0.569833   \n",
       " m-bert en + adv  0.612938  0.753839  0.621278  0.323539  0.630590  0.523289   \n",
       " \n",
       "                       avg  \n",
       " xlm-r adv        0.716068  \n",
       " m-bert adv       0.724724  \n",
       " xlm-r en + adv   0.620612  \n",
       " m-bert en + adv  0.577579  ,\n",
       " 'Semantic accuracy':                        de        es        fr        ja        pt        zh  \\\n",
       " xlm-r adv        0.537748  0.536424  0.347020  0.140397  0.536424  0.255629   \n",
       " m-bert adv       0.544371  0.550993  0.466225  0.136424  0.558940  0.286093   \n",
       " xlm-r en + adv   0.303311  0.374834  0.196026  0.007947  0.321854  0.079470   \n",
       " m-bert en + adv  0.254305  0.357616  0.241060  0.018543  0.305960  0.108609   \n",
       " \n",
       "                       avg  \n",
       " xlm-r adv        0.392274  \n",
       " m-bert adv       0.423841  \n",
       " xlm-r en + adv   0.213907  \n",
       " m-bert en + adv  0.214349  }"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# СРАВНЕНИЕ МОДЕЛЕЙ МЕЖДУ СОБОЙ ПО АТАКЕ ALIGNMENTS\n",
    "\n",
    "output = {index_renamer[key]: {} for key in index_renamer}\n",
    "\n",
    "for model_name, model_arg in zip(model_names[4:], model_args[4:]):\n",
    "    df = get_model_attacks('en', model_name.split()[0], *model_arg)\n",
    "\n",
    "    for key in index_renamer.keys():\n",
    "        mask = df.index.map(lambda x: 'Align' in x)\n",
    "        values = df[mask][key].to_dict()\n",
    "        values['[avg]'] = np.mean(list(values.values()))\n",
    "        output[index_renamer[key]][model_name] = {key_[key_.find('[') + 1:key_.find(']')]: values[key_] for key_ in values.keys()}\n",
    "\n",
    "output = {key: pd.DataFrame.from_dict(output[key]).rename(index=index_renamer).transpose() for key in output.keys()}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T18:57:19.021485Z",
     "start_time": "2021-05-15T18:57:19.018914Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + '6.tex', 'w') as f:\n",
    "    for key in output.keys():\n",
    "        print(\n",
    "            df_to_latex(\n",
    "                output[key],\n",
    "                f'Сравнение моделей с защитой между собой после phrase-level атаки на тестовую выборку датасета MultiAtis++ по метрике \\\\textbf{{{key}}}. По колонкам встраиваемые языки, по рядам тестируемые модели.'\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
