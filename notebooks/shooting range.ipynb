{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "entire-discrimination",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    " - Диалоговый датасет\n",
    "     - BANKING77\n",
    "     - CLINC150\n",
    "     - HWU64\n",
    "     - https://github.com/google-research-datasets/dstc8-schema-guided-dialogue\n",
    " - Генератор (XLM-R)\n",
    "     - придумать как генерировать токен на другом языке\n",
    "     - применить к задачке\n",
    " - LABSE для полученных и исходных предложений\n",
    "     - сделать кастомный класс лосса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-immune",
   "metadata": {},
   "source": [
    "Для XLM-R:\n",
    "\n",
    "Занулить веса на выходе для всех языков кроме русского (пройтись по всем токенам и регулярочкой выявить те токены, которые относятся к русскому). Перед backward занулять градиенты по всем токенам не из русского, ибо торч не даст ставить requires_grad на отдельную часть тензора.\n",
    "\n",
    "Как вариант - написать обертку над моделью, которая будет иметь меньший размер выходной головы, потом делать новый тензор с нулями на месте других языков и выходом модели для русского."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demographic-lotus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T11:39:59.794931Z",
     "start_time": "2021-02-13T11:39:57.710363Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "\n",
    "sns.set(style='darkgrid', rc={'figure.figsize': (16, 9)})\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "thick-dance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T12:35:35.137547Z",
     "start_time": "2021-02-13T12:35:34.992005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61767, 46)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/dstc_utterances.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts, intents = zip(*((elem['text'], elem['intent']) for elem in data))\n",
    "\n",
    "len(texts), len(np.unique(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "systematic-iraqi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T12:35:35.756333Z",
     "start_time": "2021-02-13T12:35:35.750853Z"
    }
   },
   "outputs": [],
   "source": [
    "lens = [len(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rough-harvest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T12:36:01.751131Z",
     "start_time": "2021-02-13T12:36:01.475080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unable-restaurant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:12:34.835627Z",
     "start_time": "2021-02-17T09:12:16.899589Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "heavy-harris",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:26:49.929658Z",
     "start_time": "2021-02-17T09:26:49.927826Z"
    }
   },
   "outputs": [],
   "source": [
    "def parameters(model):\n",
    "    yield from model.roberta.parameters()\n",
    "    for (name, parameter) in model.lm_head.named_parameters():\n",
    "        if name != 'decoder.weight' and name != 'bias':\n",
    "            yield parameter\n",
    "        else:\n",
    "            print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brutal-wilderness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:26:50.783217Z",
     "start_time": "2021-02-17T09:26:50.394713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1630,  0.1508,  0.1604,  ...,  0.1351,  0.1860,  0.0673],\n",
      "        [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078],\n",
      "        [ 0.2046,  0.2549,  0.1329,  ...,  0.1738,  0.0246,  0.2510],\n",
      "        ...,\n",
      "        [ 0.3828, -0.4441,  0.1398,  ...,  0.2268,  0.0560,  0.1038],\n",
      "        [ 0.0205, -0.1243,  0.0191,  ..., -0.0143,  0.0376, -0.1144],\n",
      "        [ 0.1009,  0.0588,  0.0551,  ...,  0.1232, -0.0064,  0.1216]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "biblical-fisher",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:56:36.802223Z",
     "start_time": "2021-02-17T09:56:36.799279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.8770, 1.4074, 1.4613, 0.5121],\n",
       "        [0.7128, 1.1528, 1.4373, 0.6964],\n",
       "        [0.9427, 0.5338, 1.2050, 0.6425],\n",
       "        [1.2502, 1.4358, 0.8711, 1.4354],\n",
       "        [0.7329, 0.7848, 0.5755, 0.6318]], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "net = torch.nn.Linear(4, 5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.weight += torch.ones_like(net.weight)\n",
    "    \n",
    "net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "split-colleague",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T10:01:17.185431Z",
     "start_time": "2021-02-17T10:01:17.183006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f476d5738e0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mult = torch.zeros_like(net.weight)\n",
    "weight_mult[[True, False, True, False, False]] = 1\n",
    "net.weight.register_hook(lambda grad: grad.mul_(weight_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "advance-shopper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T10:01:17.727189Z",
     "start_time": "2021-02-17T10:01:17.724922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "rocky-brick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:57:43.154932Z",
     "start_time": "2021-02-17T09:57:43.152712Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "optimizer.zero_grad()\n",
    "loss = net(torch.ones(4)).sum()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "hawaiian-blame",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:57:43.573501Z",
     "start_time": "2021-02-17T09:57:43.571231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.8770, 1.4074, 1.4613, 0.5051],\n",
       "        [0.7128, 1.1528, 1.4373, 0.6894],\n",
       "        [0.9427, 0.5338, 1.2050, 0.6355],\n",
       "        [1.2502, 1.4358, 0.8711, 1.4284],\n",
       "        [0.7329, 0.7848, 0.5755, 0.6248]], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "boolean-going",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:58:34.844159Z",
     "start_time": "2021-02-17T09:58:34.837464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0508e-02, -1.2433e-01,  1.9150e-02,  1.1652e-01,  2.1863e-01,\n",
       "         1.1920e-01, -2.1240e-02, -6.6406e-02, -8.9172e-02,  5.0468e-03,\n",
       "        -6.1157e-02,  5.4047e-02,  7.9895e-02,  1.0938e-01, -6.5063e-02,\n",
       "        -1.7593e-02,  1.0699e-01,  2.4063e-02, -7.1960e-02,  5.3375e-02,\n",
       "         1.1884e-01,  1.0843e-03, -1.5979e-01,  1.8335e-01, -1.0907e-01,\n",
       "         8.0017e-02,  7.4219e-02,  1.2708e-01,  3.3081e-02,  1.2384e-01,\n",
       "        -5.8289e-02,  3.3844e-02,  9.6252e-02,  1.4575e-01,  2.0523e-02,\n",
       "         5.7617e-02, -2.2720e-02,  1.3145e-02, -1.3586e-01, -8.4656e-02,\n",
       "         8.0017e-02, -2.0538e-02, -4.8401e-02,  6.2561e-02,  6.2164e-02,\n",
       "        -9.3323e-02, -4.3030e-02, -3.7567e-02, -8.0750e-02,  1.2671e-01,\n",
       "        -5.9357e-02,  7.8064e-02,  2.4170e-01, -7.2754e-02, -8.6899e-03,\n",
       "        -6.4545e-03, -1.9287e-02,  8.8318e-02, -8.7036e-02,  6.6223e-02,\n",
       "        -4.3060e-02,  1.1765e-02,  7.5439e-02, -7.9880e-03, -5.4932e-02,\n",
       "        -8.4595e-02, -7.2815e-02,  1.2445e-01,  1.3647e-01, -1.3855e-01,\n",
       "        -9.3307e-03,  6.8481e-02, -8.0017e-02,  1.2494e-01, -2.0248e-02,\n",
       "        -1.4236e-02,  9.3689e-02, -1.2527e-02, -7.6782e-02, -1.2230e-02,\n",
       "        -9.7534e-02,  1.1987e-01,  3.9276e-02,  1.7786e-01,  3.0975e-02,\n",
       "        -3.3081e-02, -9.7046e-02,  9.1003e-02,  1.3025e-01,  1.2598e-01,\n",
       "        -6.3354e-02,  1.2842e-01,  1.4244e-02,  9.6130e-02,  1.0760e-01,\n",
       "         1.4929e-01,  1.1450e-01, -1.2598e-01,  1.0059e-01,  4.2725e-02,\n",
       "         6.7139e-02, -9.3079e-02, -1.2225e-01,  6.4514e-02, -7.3608e-02,\n",
       "        -1.0083e-01,  1.4448e-03, -1.5854e-02,  1.2646e-01,  3.9215e-03,\n",
       "         1.3391e-01,  7.5256e-02,  1.5125e-01,  2.7893e-02, -1.3660e-01,\n",
       "         7.1655e-02, -1.0980e-01,  6.5674e-02,  8.2825e-02,  1.1176e-01,\n",
       "         7.8491e-02,  1.3281e-01, -6.8787e-02,  1.2805e-01, -1.3757e-01,\n",
       "        -3.6835e-02, -7.0419e-03,  7.0496e-02,  6.0059e-02,  2.6989e-03,\n",
       "         6.8665e-02, -1.7197e-02,  1.5182e-02, -1.8051e-02, -7.9468e-02,\n",
       "        -7.4158e-02,  8.0383e-02,  1.2573e-01, -3.0930e-02,  1.3770e-01,\n",
       "         1.4233e-01, -1.0217e-01, -2.7908e-02, -2.7573e-02, -1.2903e-01,\n",
       "        -1.2976e-01,  4.4769e-02,  9.0698e-02,  8.0688e-02,  1.6345e-01,\n",
       "         6.9275e-02, -1.2213e-01, -2.1558e-01,  6.5735e-02, -5.4504e-02,\n",
       "         3.9032e-02,  1.2512e-01,  8.1604e-02,  1.4453e-01, -7.4158e-02,\n",
       "         1.5282e-02,  4.0375e-02, -1.1975e-01,  6.5552e-02,  1.7410e-02,\n",
       "         1.7993e-01, -3.7659e-02,  1.0944e-01, -2.3087e-02,  1.2482e-01,\n",
       "         1.2732e-01,  1.8829e-02, -7.2266e-02, -9.5825e-02,  3.1590e-04,\n",
       "        -6.9214e-02, -1.5161e-01,  5.1971e-02, -1.4185e-01, -6.8420e-02,\n",
       "         9.8572e-02, -1.2537e-01,  1.8234e-02,  7.5684e-02, -1.4111e-01,\n",
       "        -1.2659e-01, -4.1962e-02, -1.4136e-01,  1.5717e-02, -1.2891e-01,\n",
       "        -2.0421e-04,  6.6223e-02,  6.7139e-03,  1.7670e-02, -1.2073e-01,\n",
       "         1.3379e-01, -7.4219e-02, -1.1371e-01, -1.3806e-01, -4.8157e-02,\n",
       "        -1.3574e-01, -6.4453e-02, -7.6866e-03,  1.0114e-01, -7.5378e-02,\n",
       "         4.5807e-02,  4.6204e-02,  7.9102e-02,  8.2275e-02,  6.7810e-02,\n",
       "        -6.4575e-02,  6.2744e-02,  7.3975e-02,  1.8237e-01,  6.3965e-02,\n",
       "         2.5635e-01,  6.8665e-02,  1.1401e-01,  1.5295e-01, -6.8237e-02,\n",
       "         1.4636e-01, -4.5898e-02, -1.2091e-01, -7.7209e-02, -1.2512e-01,\n",
       "        -4.5135e-02,  7.4402e-02, -8.7585e-02,  1.7181e-02, -1.2781e-01,\n",
       "        -3.9032e-02,  7.4341e-02,  1.5002e-01, -6.3232e-02,  6.5002e-02,\n",
       "        -1.1505e-01, -6.4331e-02,  4.5410e-01,  1.2286e-01, -1.1292e-01,\n",
       "         1.2549e-01, -7.9651e-03,  2.6679e-04, -9.7290e-02, -6.5857e-02,\n",
       "        -6.3965e-02, -1.3416e-01,  5.0690e-02,  6.7139e-02, -1.8280e-02,\n",
       "         7.7576e-02,  1.1520e-02, -6.4880e-02,  9.3506e-02, -8.9478e-02,\n",
       "        -4.7699e-02, -6.2500e-02,  1.5182e-02,  2.2583e-03, -1.5976e-02,\n",
       "        -1.3501e-01, -2.5284e-02,  3.0869e-02,  4.7089e-02,  8.0566e-02,\n",
       "        -5.3345e-02,  1.2708e-01,  2.5391e-01,  6.4819e-02, -1.2421e-01,\n",
       "         1.3660e-01,  1.0052e-01, -2.8687e-02,  1.2622e-01, -1.5088e-01,\n",
       "         5.6946e-02,  1.2744e-01,  4.7546e-02,  8.0383e-02,  6.2317e-02,\n",
       "        -9.2102e-02, -5.3253e-03, -1.3110e-01,  6.5842e-03, -1.2598e-01,\n",
       "        -1.3306e-01, -6.5369e-02,  6.0974e-02, -6.4201e-03, -1.2427e-01,\n",
       "        -1.0016e-01, -1.0040e-01, -3.4973e-02,  2.8833e-01,  7.1106e-02,\n",
       "        -3.9337e-02,  1.9543e-01,  5.0262e-02,  4.2908e-02, -6.9092e-02,\n",
       "        -6.2225e-02, -8.9035e-03,  1.3611e-01, -6.9458e-02,  5.6763e-02,\n",
       "        -1.2817e-01, -7.8857e-02, -1.0370e-01, -2.9617e-02,  7.3364e-02,\n",
       "         3.9825e-02,  3.7476e-02,  1.3989e-01,  1.4595e-02, -7.0435e-02,\n",
       "        -1.1566e-01,  6.0730e-02, -6.2683e-02,  5.7343e-02,  5.1361e-02,\n",
       "         6.0699e-02, -1.1157e-01,  5.2338e-02,  6.3904e-02,  4.8004e-02,\n",
       "         1.3721e-01, -4.8553e-02,  3.1494e-02,  1.4453e-01,  2.8491e-01,\n",
       "        -6.6528e-02,  8.4717e-02,  7.5867e-02, -7.3914e-02,  1.2830e-01,\n",
       "         2.8345e-01, -1.4819e-01, -6.1035e-02, -3.2745e-02,  1.1908e-01,\n",
       "         8.0383e-02,  1.7899e-02,  8.3130e-02,  8.4595e-02, -6.1340e-02,\n",
       "         2.5360e-02,  6.8420e-02, -1.9073e-02, -3.9642e-02,  8.9951e-03,\n",
       "         6.2927e-02,  1.2512e-01, -1.2549e-01,  1.1072e-01,  7.8613e-02,\n",
       "         4.7119e-02,  5.7129e-02, -3.4515e-02, -4.1313e-03, -6.6956e-02,\n",
       "        -1.0834e-01,  7.7942e-02, -1.0101e-01, -1.5282e-02, -9.2834e-02,\n",
       "         2.5610e-01, -7.4036e-02,  1.7288e-02,  2.3560e-01,  1.8616e-01,\n",
       "         1.4233e-01,  1.2192e-02, -1.2830e-01,  3.0182e-02,  2.2629e-02,\n",
       "        -7.7148e-02, -1.2830e-01,  3.4729e-02, -8.0139e-02,  9.1492e-02,\n",
       "         1.7349e-02,  1.2250e-01,  3.1250e-02,  6.2500e-02,  3.6041e-02,\n",
       "         1.2421e-02,  1.7664e-01,  1.4582e-03,  2.6688e-02, -6.9397e-02,\n",
       "         1.2708e-01,  5.6427e-02,  1.0138e-03,  9.4452e-03, -4.1077e-02,\n",
       "         1.7188e-01,  2.9126e-01, -4.7333e-02, -3.1525e-02,  9.4177e-02,\n",
       "        -3.9398e-02,  7.3303e-02,  1.5686e-02, -4.1718e-02,  1.5552e-01,\n",
       "         2.9114e-02,  5.4352e-02,  6.2683e-02, -1.6541e-02, -3.7994e-02,\n",
       "         8.4229e-02, -6.0181e-02, -6.9885e-02,  2.5803e-02, -1.6101e-01,\n",
       "        -3.2806e-02, -5.5878e-02, -4.5746e-02,  1.0663e-01, -5.2567e-03,\n",
       "         1.1765e-02,  1.6541e-01,  8.9539e-02, -1.2598e-01, -1.2646e-01,\n",
       "        -9.3689e-02,  1.3123e-01, -8.3801e-02, -7.6355e-02, -8.2703e-02,\n",
       "         8.4778e-02, -7.3792e-02, -6.6467e-02, -2.7557e-02,  5.9223e-04,\n",
       "        -6.8604e-02, -1.1896e-01,  4.7791e-02,  1.3098e-01, -9.6375e-02,\n",
       "         1.3550e-01,  7.4280e-02, -1.1707e-01, -1.6068e-02,  1.6617e-02,\n",
       "         2.2934e-02, -3.8025e-02, -8.0444e-02, -3.4393e-02, -6.4850e-03,\n",
       "        -4.1779e-02, -4.8920e-02, -3.6011e-02, -1.2952e-01, -6.4148e-02,\n",
       "        -8.0811e-02, -4.3106e-03,  2.2339e-02,  2.6505e-02,  5.5908e-02,\n",
       "        -5.0018e-02,  6.8550e-03,  8.4290e-02,  1.6077e-01, -4.8180e-03,\n",
       "         6.1340e-02, -1.9943e-02,  1.0461e-01, -1.1505e-01,  1.1560e-01,\n",
       "        -2.8824e-02, -5.2673e-02,  6.0272e-02,  8.2947e-02,  6.1890e-02,\n",
       "        -4.1595e-02, -6.3477e-02, -7.3433e-03,  2.5391e-02, -9.0515e-02,\n",
       "        -2.4261e-02, -1.8982e-02, -4.4312e-02,  1.3489e-01, -1.2219e-01,\n",
       "        -1.3586e-01, -1.5602e-03,  1.1670e-01,  1.3806e-01,  1.3336e-02,\n",
       "        -8.1848e-02,  1.3794e-01,  2.5436e-02, -6.9458e-02, -7.1777e-02,\n",
       "         1.1237e-01,  1.4319e-01,  1.5137e-01,  1.4023e-02,  6.5369e-02,\n",
       "        -1.5020e-03,  7.8186e-02, -3.2013e-02, -7.3486e-02,  1.0419e-01,\n",
       "        -6.7688e-02,  1.6687e-01,  9.2773e-02,  1.1945e-01,  8.8501e-02,\n",
       "        -6.9458e-02,  1.0883e-01, -1.3135e-01, -9.1553e-04, -6.6345e-02,\n",
       "        -7.6294e-02, -9.1858e-02, -2.8732e-02, -8.3208e-05, -7.4463e-02,\n",
       "         1.8875e-02,  7.5012e-02, -1.0818e-02, -3.1067e-02,  2.3962e-01,\n",
       "        -9.6252e-02,  1.2711e-02,  6.3843e-02,  6.4941e-02,  6.2195e-02,\n",
       "        -1.2854e-01, -4.3121e-02, -3.7903e-02,  4.0016e-03,  1.5259e-01,\n",
       "        -9.9792e-02, -1.1798e-01,  1.2457e-01,  1.2177e-01, -7.5623e-02,\n",
       "        -6.9092e-02, -1.2634e-01, -9.0271e-02, -9.6436e-03,  7.3486e-02,\n",
       "        -1.2085e-01, -8.2886e-02,  3.3600e-02,  5.9631e-02, -1.0303e-01,\n",
       "        -2.2446e-02,  1.6968e-02,  1.3196e-01,  1.2549e-01, -1.9050e-04,\n",
       "         6.2195e-02, -7.3059e-02,  5.5634e-02,  5.7800e-02, -8.0322e-02,\n",
       "        -4.1992e-02,  5.4840e-02,  5.4016e-02, -2.1362e-02, -3.9978e-02,\n",
       "         4.4708e-02, -1.6357e-02, -6.7627e-02,  8.6304e-02,  1.2390e-01,\n",
       "         2.2571e-01, -7.0190e-02,  1.4026e-01, -4.0436e-02, -9.6558e-02,\n",
       "         2.5177e-02,  9.4177e-02,  6.0349e-03,  3.1616e-02,  6.6040e-02,\n",
       "         6.2042e-02, -3.5767e-02, -1.8158e-02,  6.7749e-02,  7.6721e-02,\n",
       "        -4.7272e-02, -1.3161e-03, -1.5068e-03, -2.4304e-01,  7.0618e-02,\n",
       "         1.2268e-01,  3.8208e-02, -9.5947e-02,  1.0443e-01, -5.0934e-02,\n",
       "         1.6815e-02,  1.2964e-01, -7.0229e-03,  1.2103e-01, -5.3902e-03,\n",
       "         6.7993e-02, -1.0864e-02,  1.6003e-01,  1.7065e-01, -4.0436e-02,\n",
       "         8.3313e-02, -1.4563e-01,  7.4646e-02, -1.1194e-01,  1.3074e-01,\n",
       "        -9.4482e-02, -1.4381e-02,  2.2461e-01,  5.1025e-02, -2.4002e-02,\n",
       "         8.2947e-02,  1.6541e-02, -5.9662e-02, -3.2440e-02,  7.4219e-02,\n",
       "         1.6479e-02, -1.1975e-01,  1.5442e-01,  7.7637e-02,  1.2128e-01,\n",
       "        -9.5215e-02, -1.3525e-01,  1.6760e-01, -6.4209e-02, -7.8430e-02,\n",
       "         1.1444e-01, -2.3529e-02, -6.0120e-02,  7.9590e-02,  2.5970e-02,\n",
       "         6.3171e-02,  3.4393e-02, -3.3508e-02, -1.1609e-01, -1.2524e-01,\n",
       "         5.3345e-02,  1.3123e-01, -6.9641e-02,  5.6732e-02, -4.4556e-02,\n",
       "         1.2500e-01,  7.8064e-02, -2.7954e-02,  4.6143e-02,  1.6541e-01,\n",
       "        -6.7101e-03,  2.8381e-03,  1.3501e-01,  1.2598e-01,  1.5572e-02,\n",
       "        -1.4244e-02,  3.1830e-02, -6.9580e-02,  9.3567e-02, -6.0883e-02,\n",
       "        -5.7587e-02,  8.0185e-03,  1.6190e-02,  1.3928e-01,  2.9358e-02,\n",
       "         7.8735e-02,  1.4941e-01,  2.1744e-03,  1.3062e-01, -6.3110e-02,\n",
       "        -9.4849e-02,  1.2610e-01,  6.7749e-02,  1.2622e-01,  1.0669e-01,\n",
       "         1.0022e-01, -5.5603e-02, -4.1504e-02,  1.3379e-01,  9.7046e-02,\n",
       "         5.2795e-02, -5.7465e-02,  4.4746e-03,  6.9946e-02, -5.0323e-02,\n",
       "         1.3208e-01, -1.1713e-01,  1.2421e-01,  2.3364e-01, -5.9021e-02,\n",
       "         1.8631e-02,  6.8130e-03,  1.2219e-01, -9.4116e-02,  8.7097e-02,\n",
       "         6.8054e-02,  9.6924e-02, -1.9885e-01,  7.8247e-02,  8.8501e-02,\n",
       "        -3.1433e-02, -7.5928e-02, -6.3171e-02,  6.0242e-02,  4.3610e-02,\n",
       "         3.4668e-02, -5.2917e-02, -6.4453e-02, -1.5295e-01,  6.4201e-03,\n",
       "        -8.1665e-02, -1.1676e-01, -5.4016e-02,  3.3539e-02, -1.1255e-01,\n",
       "         1.2891e-01,  3.0502e-02, -3.6346e-02,  8.0261e-02,  7.4890e-02,\n",
       "         1.5942e-01, -7.3547e-02,  6.2805e-02,  1.6040e-01, -7.1106e-02,\n",
       "         2.5692e-03, -7.1411e-02,  3.6591e-02,  5.4199e-02, -5.8075e-02,\n",
       "         5.8167e-02,  7.9895e-02,  3.7506e-02, -1.1920e-01,  6.2500e-02,\n",
       "        -1.2878e-01, -1.3025e-01,  5.5206e-02, -6.8237e-02, -1.3477e-01,\n",
       "        -6.5804e-03, -5.5542e-02,  1.1841e-01, -1.1322e-01, -8.2458e-02,\n",
       "        -3.1097e-02,  6.4209e-02,  9.3018e-02, -3.7048e-02,  1.1368e-02,\n",
       "        -3.2471e-02,  6.2805e-02,  1.5701e-02, -6.4270e-02,  6.9153e-02,\n",
       "        -4.0894e-02,  1.1731e-01, -4.1084e-03, -5.3894e-02, -1.2573e-01,\n",
       "        -3.2837e-02, -3.6957e-02,  3.4393e-02, -2.6764e-02,  3.3783e-02,\n",
       "        -1.4320e-02,  3.7598e-02, -1.1444e-01], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.decoder.weight[250000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "published-brother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:22:42.526523Z",
     "start_time": "2021-02-17T09:22:42.524328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'dense.weight',\n",
       " 'dense.bias',\n",
       " 'layer_norm.weight',\n",
       " 'layer_norm.bias',\n",
       " 'decoder.weight']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x[0], model.lm_head.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "super-gallery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:14:59.327062Z",
     "start_time": "2021-02-13T15:14:51.807552Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), '../models/xlm-roberta-base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "specialized-rochester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:02:23.331852Z",
     "start_time": "2021-02-13T14:02:23.328964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1630,  0.1508,  0.1604,  ...,  0.1351,  0.1860,  0.0673],\n",
       "        [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078],\n",
       "        [ 0.2046,  0.2549,  0.1329,  ...,  0.1738,  0.0246,  0.2510],\n",
       "        ...,\n",
       "        [ 0.3828, -0.4441,  0.1398,  ...,  0.2268,  0.0560,  0.1038],\n",
       "        [ 0.0205, -0.1243,  0.0191,  ..., -0.0143,  0.0376, -0.1144],\n",
       "        [ 0.1009,  0.0588,  0.0551,  ...,  0.1232, -0.0064,  0.1216]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "polar-broadway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:03:19.762669Z",
     "start_time": "2021-02-13T14:03:19.760841Z"
    }
   },
   "outputs": [],
   "source": [
    "c = torch.stack((a, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "desperate-planning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:04:14.052403Z",
     "start_time": "2021-02-13T14:04:14.050851Z"
    }
   },
   "outputs": [],
   "source": [
    "emb = model.roberta.embeddings.word_embeddings.weight[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "confirmed-roman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:04:14.585771Z",
     "start_time": "2021-02-13T14:04:14.583855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 100]), torch.Size([2, 100, 768]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "revolutionary-premiere",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:04:21.177469Z",
     "start_time": "2021-02-13T14:04:21.175883Z"
    }
   },
   "outputs": [],
   "source": [
    "emb[c == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "visible-chemistry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:04:25.079999Z",
     "start_time": "2021-02-13T14:04:25.077056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0908, -0.1393, -0.0595,  ...,  0.1466, -0.0898,  0.2864],\n",
       "         [ 0.0435,  0.4148, -0.3076,  ...,  0.3025,  0.0803, -0.0367],\n",
       "         ...,\n",
       "         [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078],\n",
       "         [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078],\n",
       "         [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0908, -0.1393, -0.0595,  ...,  0.1466, -0.0898,  0.2864],\n",
       "         [ 0.0435,  0.4148, -0.3076,  ...,  0.3025,  0.0803, -0.0367],\n",
       "         ...,\n",
       "         [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078],\n",
       "         [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078],\n",
       "         [-0.0073,  0.0048, -0.0078,  ...,  0.0078,  0.0041, -0.0078]]],\n",
       "       grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daily-automation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T14:02:40.786441Z",
     "start_time": "2021-02-13T14:02:40.784025Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tokenizer.encode(\"I went fishing and I caught several big fish\", return_tensors='pt', max_length=100, padding='max_length').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lucky-receptor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:08:24.079542Z",
     "start_time": "2021-02-13T08:08:22.697094Z"
    }
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def is_cyrillic(s: str):\n",
    "    return bool(regex.search(r'\\p{IsCyrillic}', s))\n",
    "\n",
    "\n",
    "russian_tokens_mask = np.zeros(model.lm_head.decoder.weight.shape[0])\n",
    "\n",
    "for token in range(model.lm_head.decoder.weight.shape[0]):\n",
    "    if is_cyrillic(tokenizer.decode([token])):\n",
    "        russian_tokens_mask[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "communist-arcade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:08:26.012140Z",
     "start_time": "2021-02-13T08:08:26.010616Z"
    }
   },
   "outputs": [],
   "source": [
    "russian_tokens_mask = russian_tokens_mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "graphic-sellers",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:08:27.153352Z",
     "start_time": "2021-02-13T08:08:27.151314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_tokens_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conceptual-conflict",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:08:30.938037Z",
     "start_time": "2021-02-13T08:08:30.894337Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.lm_head.decoder.weight[~russian_tokens_mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "protecting-updating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:55:56.435053Z",
     "start_time": "2021-02-13T08:55:56.282812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = f'I have a {tokenizer.mask_token}'\n",
    "\n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "tokenizer.decode([torch.argmax(mask_token_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "conservative-colon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:59:38.945129Z",
     "start_time": "2021-02-13T08:59:28.870694Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lesha/.local/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:966: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "sequence = f\"I went fishing and I {tokenizer.mask_token} several big fish.\"\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "token_logits = model(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optical-utilization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:41:12.959013Z",
     "start_time": "2021-02-15T14:41:12.491011Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pointed-poland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:41:32.227113Z",
     "start_time": "2021-02-15T14:41:29.296676Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "statutory-lemon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:41:49.482942Z",
     "start_time": "2021-02-15T14:41:49.157716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/tokenizer/tokenizer_config.json',\n",
       " '../models/tokenizer/special_tokens_map.json',\n",
       " '../models/tokenizer/unigram.json',\n",
       " '../models/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('../models/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "assumed-porcelain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:01:25.903105Z",
     "start_time": "2021-02-13T09:01:15.943187Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizerFast\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "elect-nightlife",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:02:36.758987Z",
     "start_time": "2021-02-13T09:02:29.490075Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/xlm-roberta-base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "extraordinary-collection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:59:38.948156Z",
     "start_time": "2021-02-13T08:59:38.945974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went fishing and I caught several big fish.\n",
      "I went fishing and I saw several big fish.\n",
      "I went fishing and I found several big fish.\n",
      "I went fishing and I got several big fish.\n",
      "I went fishing and I had several big fish.\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "friendly-algorithm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:35:38.016118Z",
     "start_time": "2021-02-13T08:35:38.013569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         ...,\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "finite-reach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:37:27.658813Z",
     "start_time": "2021-02-13T08:37:27.657187Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = inputs.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "limiting-group",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:38:12.556829Z",
     "start_time": "2021-02-13T08:38:12.553264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(inputs.shape[0], 250002).scatter_(1, inputs, 1.0)[inputs.flatten() == tokenizer.mask_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "thirty-bearing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:38:09.498202Z",
     "start_time": "2021-02-13T08:38:09.495607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([False, False, False, False,  True, False, False, False])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inputs.flatten() == tokenizer.mask_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abandoned-labor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:14:08.098411Z",
     "start_time": "2021-02-13T08:14:08.096560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 250002])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "wrong-deputy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:20:34.395579Z",
     "start_time": "2021-02-13T08:20:34.286616Z"
    }
   },
   "outputs": [],
   "source": [
    "tensor = torch.LongTensor([1, 2, 3]).reshape(-1, 1)\n",
    "\n",
    "y = torch.zeros(len(tensor), 250002).scatter_(1, tensor, 1.0) @ model.roberta.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outer-spain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:09:49.316982Z",
     "start_time": "2021-02-13T08:09:49.277564Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_output = one_hot @ model.roberta.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aquatic-enhancement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:20:37.402986Z",
     "start_time": "2021-02-13T08:20:37.401164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "inside-expert",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:19:20.413030Z",
     "start_time": "2021-02-13T08:19:20.407930Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-18d0e326655d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "y[tensor == 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "russian-symphony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:19:44.138242Z",
     "start_time": "2021-02-13T08:19:44.136356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 768]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "elder-bullet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:21:01.467681Z",
     "start_time": "2021-02-13T08:21:01.465784Z"
    }
   },
   "outputs": [],
   "source": [
    "y = torch.zeros(len(tensor), 250002).scatter_(1, tensor, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abroad-clothing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:21:49.585592Z",
     "start_time": "2021-02-13T08:21:49.578331Z"
    }
   },
   "outputs": [],
   "source": [
    "y[tensor.flatten() == 3] = torch.nn.functional.gumbel_softmax(mask_token_logits, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bound-expert",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:21:53.889437Z",
     "start_time": "2021-02-13T08:21:53.886981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "headed-scheduling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:17:03.405171Z",
     "start_time": "2021-02-13T08:17:03.403174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "legendary-dutch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:17:08.581241Z",
     "start_time": "2021-02-13T08:17:08.492078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836],\n",
       "         [ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(inputs_embeds=y.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "accurate-federal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:17:49.733161Z",
     "start_time": "2021-02-13T08:17:49.639152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836]],\n",
       "\n",
       "        [[ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836]],\n",
       "\n",
       "        [[ 0.4143, -0.0156,  0.2747,  ...,  0.1248,  0.1586, -0.0836]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(input_ids=tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "central-short",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:35:00.859281Z",
     "start_time": "2021-02-11T16:35:00.851315Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = inputs.float()\n",
    "outputs[inputs == tokenizer.mask_token_id] = torch.sum(torch.nn.functional.gumbel_softmax(mask_token_logits, hard=True) * torch.arange(0, mask_token_logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "productive-prototype",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:35:01.207968Z",
     "start_time": "2021-02-11T16:35:01.205645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  87., 765.,  10., 983.,   6.,   5.,   2.]],\n",
       "       grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "smart-woman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:35:02.899747Z",
     "start_time": "2021-02-11T16:35:02.898262Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs2 = outputs.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "synthetic-static",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:35:41.514662Z",
     "start_time": "2021-02-11T16:35:41.512614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IndexPutBackward at 0x7fa80df76580>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "wireless-silly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:35:49.116323Z",
     "start_time": "2021-02-11T16:35:49.110502Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attribute 'grad_fn' of 'torch._C._TensorBase' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-178a35c66f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: attribute 'grad_fn' of 'torch._C._TensorBase' objects is not writable"
     ]
    }
   ],
   "source": [
    "outputs2.grad_fn = outputs.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "every-demographic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:34:32.324349Z",
     "start_time": "2021-02-11T16:34:32.322942Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "systematic-pledge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:28:49.064502Z",
     "start_time": "2021-02-11T16:28:49.062433Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "charming-dining",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:29:12.333397Z",
     "start_time": "2021-02-11T16:29:12.316782Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index put requires the source and destination dtypes match, got Long for the destination and Float for the source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-04426102ad6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgumbel_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Index put requires the source and destination dtypes match, got Long for the destination and Float for the source."
     ]
    }
   ],
   "source": [
    "inputs[inputs == tokenizer.mask_token_id] = torch.sum(torch.nn.functional.gumbel_softmax(mask_token_logits, hard=True) * torch.arange(0, mask_token_logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "turkish-shareware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:27:59.542256Z",
     "start_time": "2021-02-11T16:27:59.539202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 8.7000e+01, 7.6500e+02, 1.0000e+01, 2.0384e+05, 6.0000e+00,\n",
       "         5.0000e+00, 2.0000e+00]], grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "right-integration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:25:31.233120Z",
     "start_time": "2021-02-11T16:25:31.231193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tired-senegal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:15:46.997600Z",
     "start_time": "2021-02-13T15:15:46.994607Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss counting similarity between target and output sentence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = 'sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        outputs_1 = self.model(**outputs)\n",
    "        outputs_2 = self.model(**labels)\n",
    "\n",
    "        embeddings_1 = outputs_1.pooler_output\n",
    "        embeddings_2 = outputs_2.pooler_output\n",
    "\n",
    "        normalized_embeddings_1 = F.normalize(embeddings_1, p=2)\n",
    "        normalized_embeddings_2 = F.normalize(embeddings_2, p=2)\n",
    "\n",
    "        return normalized_embeddings_1 @ normalized_embeddings_2.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designed-museum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:15:58.150446Z",
     "start_time": "2021-02-13T15:15:47.317571Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "soviet-metropolitan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:16:11.942688Z",
     "start_time": "2021-02-13T15:16:06.013913Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/xlm-roberta-base-mean-tokens.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dental-importance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T17:23:32.445045Z",
     "start_time": "2021-02-12T17:23:32.443082Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.to_json_file('../models/xmr-roberta-base-mean-tokens.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "binary-placement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:36:42.353855Z",
     "start_time": "2021-02-13T09:36:38.961668Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, PretrainedConfig\n",
    "\n",
    "model = XLMRobertaModel(PretrainedConfig.from_json_file('../models/xlm-roberta-base-mean-tokens.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "inappropriate-worth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:36:46.912693Z",
     "start_time": "2021-02-13T09:36:46.558030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../models/xlm-roberta-base-mean-tokens.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "enabling-musical",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:36:47.515682Z",
     "start_time": "2021-02-13T09:36:47.512327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "stock-rolling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T15:52:36.863646Z",
     "start_time": "2021-02-11T15:50:51.419816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0149b7fe1d4f8c9833957aec2b93d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1112256686.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = CustomLoss(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "elect-foster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T15:53:47.205961Z",
     "start_time": "2021-02-11T15:53:45.306289Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "interpreted-optics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:47:44.370535Z",
     "start_time": "2021-02-13T09:47:44.364213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0416,  0.0110, -0.0000,  ...,  0.0000, -0.1096, -0.0141],\n",
       "         [ 0.0631, -0.6998, -0.5704,  ..., -0.1152, -0.4210,  0.5102],\n",
       "         [-0.1585,  0.0000, -0.7146,  ...,  0.6517,  0.3447, -0.1969],\n",
       "         ...,\n",
       "         [-0.0000, -0.1132, -0.4343,  ...,  1.0054, -0.0928, -0.0329],\n",
       "         [ 0.1366,  0.8943,  0.0611,  ...,  0.0495, -0.3360, -0.6585],\n",
       "         [ 0.0166,  0.1572,  0.0089,  ...,  0.0337,  0.0542, -0.1161]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"I went fishing and I caught several big fish\", return_tensors='pt')\n",
    "\n",
    "model.embeddings(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "tough-nursing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:47:45.325211Z",
     "start_time": "2021-02-13T09:47:45.321214Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "tokens_one_hot = torch.zeros(inputs.shape[0], 250002).scatter_(\n",
    "    1, inputs, 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "third-gravity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:47:45.897186Z",
     "start_time": "2021-02-13T09:47:45.818972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0416,  0.0000, -0.0678,  ...,  0.0280, -0.1096, -0.0141],\n",
       "         [ 0.0631, -0.6998, -0.5704,  ..., -0.1152, -0.4210,  0.5102],\n",
       "         [-0.1585,  0.4480, -0.7146,  ...,  0.6517,  0.3447, -0.1969],\n",
       "         ...,\n",
       "         [-0.8030, -0.1132, -0.4343,  ...,  1.0054, -0.0928, -0.0329],\n",
       "         [ 0.1366,  0.8943,  0.0611,  ...,  0.0000, -0.3360, -0.6585],\n",
       "         [ 0.0166,  0.1572,  0.0089,  ...,  0.0337,  0.0542, -0.1161]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.forward(inputs_embeds=(tokens_one_hot @ model.embeddings.word_embeddings.weight).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "speaking-second",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:48:20.833661Z",
     "start_time": "2021-02-13T09:48:20.758772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(model.embeddings(inputs), model.embeddings.forward(inputs_embeds=(tokens_one_hot @ model.embeddings.word_embeddings.weight).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "local-vulnerability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:51:28.581453Z",
     "start_time": "2021-02-13T09:51:28.518050Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer.encode(\"I went fishing and I caught several big fish\", return_tensors='pt')\n",
    "    \n",
    "    x = model.embeddings(inputs)\n",
    "    \n",
    "    inputs = tokenizer.encode(\"I went fishing and I caught several big fish\", return_tensors='pt').reshape(-1, 1)\n",
    "\n",
    "    tokens_one_hot = torch.zeros(inputs.shape[0], 250002).scatter_(\n",
    "        1, inputs, 1.0\n",
    "    )\n",
    "    \n",
    "    y = model.embeddings.forward(inputs_embeds=(tokens_one_hot @ model.embeddings.word_embeddings.weight).unsqueeze(0))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "confirmed-ethnic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:51:30.257545Z",
     "start_time": "2021-02-13T09:51:30.255685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "electric-gauge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:51:31.044750Z",
     "start_time": "2021-02-13T09:51:31.042902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "minimal-integer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T09:51:31.736178Z",
     "start_time": "2021-02-13T09:51:31.733907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((x == y).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-hunger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "diploma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
