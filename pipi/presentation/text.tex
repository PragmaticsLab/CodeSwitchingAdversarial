\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{indentfirst}

\makeatletter
\renewcommand{\@biblabel}[1]{#1.}
\makeatother

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\setbeamertemplate{bibliography item}[text]
\mode<presentation> {

\usetheme[compress]{Singapore}
\usecolortheme{orchid}
}

\setcounter{tocdepth}{1}


\useinnertheme{rounded}
\let\oldfootnote\footnote
\renewcommand\footnote[1][]{\oldfootnote[frame,#1]}

\usepackage{graphicx}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Adversarial Attacks on Cross-lingual Models}

\author{Birshert Alexey}
\date{\today}

\begin{document}

\begin{frame}
Good morning dear committee. My name is Alexey Birshert, and I am about to demonstrate my research called "Adversarial attacks on cross-lingual models" to you.
\end{frame}

\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
\setbeamertemplate{subsection in toc}{\qquad~\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\tiny}

%------------------------------------------------
\section{INTRODUCTION}
%------------------------------------------------

\begin{frame}
\frametitle{Code-switching}
I am researching natural language processing, more precisely the multilingual text corpora analysis and such a phenomenon as code-switching. Code-switching occurs in multilingual communities around the world and ordinary texts on the Internet, social media. It consists of mixing and using two or more languages within one phrase or sentence. In my work, I set myself to solve the problem of generating realistic texts with code-switching. Such texts’ artificial creation would improve the existing datasets for classic multilingual tasks like recognizing user intent or question-answering systems. It would enhance language models’ quality and make them more resistant to code-switching and more robust.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Texts with code-switching generation}
For data generation, I would construct a language model based on Transformer architecture. I plan to utilize three different architectures - XLM, XLM-R and BERT. To train such a model, I would use adversarial attacks.
\end{frame}

%------------------------------------------------

%------------------------------------------------
\section{METHODS}
%------------------------------------------------

\begin{frame}
\frametitle{Training procces}
As the primary model archetype, I have a neural network that fills ”slots” in sentences. As an input, it takes a sentence with some tokens changed to the special mask token. As an output, it generates a probability distribution over tokens’ vocabulary. I want to train the model in such a way that it would result in increasing other languages probabilities.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Training procces}
Initially, these language models are trained on the data collected on the Internet, such as Wikipedia and books. These sources are primarily monolingual. The probability distribution over tokens that are not from the sentence's source language is very close to uniform, and every token probability is very close to zero. Here you can see a density plot of probabilities generated by the "slot" filling language model.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Training procces}
To train a language model to generate tokens from the target language, I would use a complex loss function. It would consist of two components. First - I want to keep the meaning of the original sentence. I would calculate the semantic distance between the original sentence and the generated one. Second - I want to keep the classifier score for sentence classification. I would estimate the difference between the original score and the generated score. 
\end{frame}

%------------------------------------------------


\begin{frame}
\frametitle{Experiments}
I project to experiment with changing the training process. In some experiments, I would try to determine how both loss function parts influence the overall performance. In other experiments, I would try to determine how to choose tokens to mask. Mainly all current approaches base on linguistic semantic constraints - the goal is to generate realistic code-switching data. The insertions could be words or more significant constituents, and they would comply with the grammatical frame of the language. However, random word insertions could lead to the formation of unnatural code-mixed sentences, which are very rare in practice.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Evaluation}
After training, I would evaluate my experiments and resulting models by two types of metrics. First - I would measure code-switching specific metrics that cover the quality and realism of the generated text. Second - I would finetune a language model for the user-intent classification task and compare the overall performance.
\end{frame}

%------------------------------------------------

%------------------------------------------------
\section{RESULTS ANTICIPATED}
%------------------------------------------------

\begin{frame}
The research’s main expected result is a training algorithm for the Transformer type neural networks to generate realistic code-switching data. A trained model should be capable of generating multilingual code-mixed training data for the question-answering task. It also should achieve higher metrics values than existing methods.
\end{frame}

%------------------------------------------------

%------------------------------------------------
\section{CONCLUSION}
%------------------------------------------------

\begin{frame}
\frametitle{Future plans}
This research should help classical language models become more robust and work better with multilingual texts. Further research on the topic of code-switching can use the resulting model for conducting various experiments.
\\
In the future, I plan to experiment with classical language models fine-tuned on the generated data and improve their quality on particular benchmarks.
\end{frame}

%------------------------------------------------

\subsection{That's all}
\begin{frame}
\begin{figure}
	\includegraphics[width=0.9\linewidth]{images/end.png}
\end{figure}
\end{frame}

%------------------------------------------------

%------------------------------------------------

\section{REFERENCES} 

\begin{frame}
\frametitle{}
\begin{thebibliography}{0}

\bibitem{transformer} ``Attention Is All You Need.`` arXiv preprint arXiv:1706.03762v5.

\bibitem{dilma} ``Differentiable Language Model Adversarial Attacks on Categorical Sequence Classifiers.`` arXiv preprint arXiv:2006.11078.

\bibitem{lstm} ``A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning.`` Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2267–2280

\bibitem{glue} ``DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue.`` arXiv preprint arXiv:2009.13570v2.

\bibitem{xlm} Guillaume Lample, Alexis Conneau. 2019. ``Cross-lingual Language Model Pretraining.`` arXiv preprint arXiv:1901.07291.

\bibitem{xlmr} ``Unsupervised Cross-lingual Representation Learning at Scale.`` arXiv preprint arXiv:1911.02116v2.

\end{thebibliography}
\end{frame}

\end{document} 