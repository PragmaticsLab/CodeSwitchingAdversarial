# DIPLOMA

This is a repo for publishing my current diploma work.

There is a common phenomenon in multilingual societies all around the world code-mixing, it consists in mixing different languages inside one utterance.
Multilingual models have demonstrated incredible performance in various natural language processing tasks.
However, real code-mixing data is very expensive to collect and label.
We present two gray-box adversarial attacks, build to evaluate multilingual language models capacity to work with code-mixing input data.
Additionally we present an adversarial pretraining method to make the models more robust to attacks.

In our work we solve the joint slot-filling and intent recognition task with 98\% intent accuracy and 95\% slots F1 score;
bring models performance down from 78\% to 16\% in semantic accuracy metric with adversarial attack;
increase models performance from 8.8\% to 20\% in semantic accuracy metric with proposed protection method.

Keywords - Joint slot-filling and intent recognition, adversarial attacks, multilingual language models, adversarial training.
