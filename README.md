# DIPLOMA

This is a repo for publishing my current diploma work.

Multilingual models have demonstrated incredible cross-lingual performance.
However, datasets like MultiAtis++ stay monolingual at the example level.
There is a common phenomenon in multilingual societies all around the world code-mixing, it consists in mixing different languages inside one utterance.
In this work we present two gray-box adversarial attacks, build to evaluate multilingual language models capacity to work with code-mixing input data.
Additionally we present an adversarial pretraining method to make the models more robust to attacks.
In our work we solve the joint slot-filling and intent recognition task with 98\% intent accuracy and 95\% slots F1 score;
bring models performance down from 78\% to 16\% in semantic accuracy metric with adversarial attack;
increase models performance from 8.8\% to 20\% in semantic accuracy metric with proposed protection method.

Keywords - Joint slot-filling and intent recognition, adversarial attacks, multilingual language models, adversarial training.
