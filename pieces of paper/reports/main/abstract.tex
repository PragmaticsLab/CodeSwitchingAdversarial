\begin{abstract}
    Мультиязычные модели показывают удивительную способность к переносу знаний.
    Однако наборы данных вроде MultiAtis++ остаются моноязычными на уровне примеров.
    В мультиязычных сообществах по всему миру распространён феномен смешения кодов, когда человек использует в речи более одного языка внутри одного предложения.
    Мы представляем две адверсариальные атаки по методу серого ящика, чтобы оценить возможности мультиязычных моделей к работе со смешением кодов.
    Дополнительно мы предлагаем метод адверсариального предобучения для защиты от атак.
    В своей работе мы решаем задачу одновременного заполнения слотов и распознавания интентов с качеством 98\% accuracy по интентам и 95\% F1 меры по слотам;
    понижаем качество моделей с 78\% до 16\% по метрике semantic accuracy с помощью адверсариальной атаки;
    повышаем качество моделей с 8.8\% до 20\% по метрике semantic accuracy с помощью предложенного метода защиты.
    \newline
    Ссылка на гитхаб с проектом - \url{https://github.com/birshert/attack-lang-models}.
    \newline
    \small \textbf{\textit{Ключевые слова---}}Одновременное заполнение слотов и распознавание интентов, адверсариальные атаки, мультиязычные языковые модели, адверсариальное обучение
    \newline
    \newline
    Multilingual models have demonstrated incredible cross-lingual performance.
    However, datasets like MultiAtis++ stay monolingual at the example level.
    There is a common phenomenon in multilingual societies all around the world code-mixing, it consists in mixing different languages inside one utterance.
    In this work we present two gray-box adversarial attacks, build to evaluate multilingual language models capacity to work with code-mixing input data.
    Additionally we present an adversarial pretraining method to make the models more robust to attacks.
    In our work we solve the joint slot-filling and intent recognition task with 98\% intent accuracy and 95\% slots F1 score;
    bring models performance down from 78\% to 16\% in semantic accuracy metric with adversarial attack;
    increase models performance from 8.8\% to 20\% in semantic accuracy metric with proposed protection method.

    \newline
    Github project link - \url{https://github.com/birshert/attack-lang-models}.
    \newline
    \small \textbf{\textit{Keywords---}}Joint slot-filling and intent recognition, adversarial attacks, multilingual language models, adversarial training
\end{abstract}
