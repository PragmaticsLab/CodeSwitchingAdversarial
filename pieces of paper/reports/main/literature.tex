\subsection{Мультиязычные модели}
Языки с небольшим количеством данных часто не могут предоставить достаточного размера датасета для обучения с учителем.
Существует подход для борьбы с этим, который заключается в построении кросс-язычных представлений.
Эти представления нужно дообучать для специфичной задачи на языке с большим количеством ресурсов, чтобы показывать хорошее качество на других, менее ресурсоёмких языках~\cite{klementiev-etal-2012-inducing}.
\parВслед за успехом модели Трансформер~\cite{Vaswani2017AttentionIA},
недавние мультиязычные модели такие как m-BERT~\cite{devlin-etal-2019-bert} и XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR}
переносят парадигму <<предобучение $\rightarrow$ дообучение под специфическую задачу>> в мультиязычную область.
Они предобучают энкодеры на основе архитектуры Трансформера на текстовых данных с различными задачами языкового моделирования.
Затем эти предобученные энкодеры могут быть дообучены для конкретной задачи на ресурсоёмком языке для которого есть много размеченных данных.
Это известно как кросс-язычный перенос знаний.
\parВ одних недавних исследованиях кросс-язычного переноса знаний было показано, что качество модели на ранее не виденных тестовых языках сильно зависит от количества обучающих данных и размера контекста~\cite{Liu2020WhatMM}.
В~\cite{Wu2019BetoBB} было показано, что m-BERT показывает очень сильную способность к кросс-язычному переносу знаний.
m-BERT превосходит по качеству мультиязычные эмбеддинги в четырёх из пяти исследуемых задач без какой-либо информации о связи языков.
\par Более современная и более сложная модель XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR} показывает лучшее, чем m-BERT качество, однако требует массивных объемов обучающих данных для хорошей работы.
В своём исследовании авторы XLM-RoBERTa показывают, что их модель является самой сильной мультиязычной моделью на текущий момент.
\par m-BERT обучается на корпусах Wikipedia и Books, в то время как XLM-RoBERTa обучается на CommonCrawl, который содержит для многих языков на несколько порядков больше данных.

\subsection{Классификация интентов и заполнение слотов}
Повсеместное использование виртуальных ассистентов постепенно становится ежедневной реальностью с ростом их популярности.
Богатство возможностей и качество работы ассистента напрямую влияет на удобство его использования.
Хорошие ассистенты будут привлекать всё больше людей, занимая доли рынка.
Ключевым аспектом в работе виртуального помощника является правильная классификация интентов и заполнение слотов в запросах.
Интент — это желаемый результат запроса пользователя.
Слоты — это слова или наборы слов, которые содержат релевантную интенту информацию.
\parИз-за тесной корреляции между задачами заполнения слотов и классификации интентов обычно используется одна модель для одновременного решения обеих задач~\cite{Weld2021ASO}.
Актуальные подходы последнего времени используют модели на основе Трансформера, например BERT~\cite{devlin-etal-2019-bert}.
Одним из популярных датасетов для этой задачи является датасет MultiAtis++~\cite{Xu2020EndtoEndSA}.

\subsection{Машинный перевод и выравнивание слов}
Для машинного перевода в своей работе мы будем использовать~\cite{Fan2020BeyondEM}.
Созданная авторами статьи модель обучалась на внушительном датасете из 7.5 миллиардов предложений для 100 языков.
Данная модель основна на архитектуре Трансформера и способна переводить с любого на любой язык в пределе ста обучающих.
На текущий момент это одна из самых сильных моделей для машинного перевода, которая успешно справляется с переводом на любые, даже ранее низкоресурсные, языки.
\parДля построения выравниваний между параллельными предложениями на разных языках мы будем использовать~\cite{Dou2021WordAB}.
Оригинальный подход авторов статьи использует эмбеддинги от мультиязычной языковой модели m-BERT~\cite{devlin-etal-2019-bert}.
Среди результатов постулируется превосходство данного подхода над всеми остальными на текущий момент.

\subsection{Смешение кодов в адверсариальных атаках на мультиязычные модели}
Основная - ~\cite{Tan2021CodeMixingOS}.
Побочная - ~\cite{Krishnan2021MultilingualCF}.
Пуперпобочная - ~\cite{santy-etal-2021-bertologicomix}.

% TODO: подробно расписать че происходит в первой и второй статьях, описать, что мы решили сделать что-то своё на основе первой статьи
% TODO: + попробовать предложить свой собственный метод защиты.
