\subsection{Мультиязычные модели}
Языки с небольшим количеством данных часто не могут предоставить достаточного размера датасета для обучения с учителем.
Существует подход для борьбы с этим, который заключается в построении кросс-язычных представлений.
Эти представления нужно дообучать для специфичной задачи на языке с большим количеством ресурсов, чтобы показывать хорошее качество на других, менее ресурсоёмких языках~\cite{klementiev-etal-2012-inducing}.
\parВслед за успехом модели Трансформер~\cite{Vaswani2017AttentionIA},
недавние мультиязычные модели такие как m-BERT~\cite{devlin-etal-2019-bert} и XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR}
переносят парадигму <<предобучение $\rightarrow$ дообучение под специфическую задачу>> в мультиязычную область.
Они предобучают энкодеры на основе архитектуры Трансформера на текстовых данных с различными задачами языкового моделирования.
Затем эти предобученные энкодеры могут быть дообучены для конкретной задачи на ресурсоёмком языке для которого есть много размеченных данных.
Это известно как кросс-язычный перенос знаний.
\parВ одних недавних исследованиях кросс-язычного переноса знаний было показано, что качество модели на ранее не виденных тестовых языках сильно зависит от количества обучающих данных и размера контекста~\cite{Liu2020WhatMM}.
В~\cite{Wu2019BetoBB} было показано, что m-BERT показывает очень сильную способность к кросс-язычному переносу знаний.
m-BERT превосходит по качеству мультиязычные эмбеддинги в четырёх из пяти исследуемых задач без какой-либо информации о связи языков.
\par Более современная и более сложная модель XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR} показывает лучшее, чем m-BERT качество, однако требует массивных объемов обучающих данных для хорошей работы.
В своём исследовании авторы XLM-RoBERTa показывают, что их модель является самой сильной мультиязычной моделью на текущий момент.
\par m-BERT обучается на

% TODO: написать про то как обучается берт и хлмр, дописать что-нибудь еще про хлмр что он силён.

\subsection{Классификация интентов и заполнение слотов}
~\cite{Weld2021ASO}

% TODO: написать, что есть задача, её в основном решают вот так вот (Невский), мы будем так же решать, в целом есть вот такие интересные приколдессы тут.

\subsection{Смешение кодов в адверсариальных атаках на мультиязычные модели}
Основная - ~\cite{Tan2021CodeMixingOS}.
Побочная - ~\cite{Krishnan2021MultilingualCF}.
Пуперпобочная - ~\cite{santy-etal-2021-bertologicomix}.

% TODO: подробно расписать че происходит в первой и второй статьях, описать, что мы решили сделать что-то своё на основе первой статьи
% TODO: + попробовать предложить свой собственный метод защиты.

\subsection{Машинный перевод и выравнивание слов}
Перевод -~\cite{Fan2020BeyondEM}.
\parВыравнивание - ~\cite{Dou2021WordAB}.

% TODO: черкануть про то как вообще это всё работает учится и что это соты на текущий момент а не хухры мухры вам тут.
